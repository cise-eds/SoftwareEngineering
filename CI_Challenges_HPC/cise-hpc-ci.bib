%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Gamblin, Todd at 2022-10-09 18:17:39 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{ames-hybrid-cloud,
	abstract = {This work investigates the feasibility of a Singularity container-based solution to support a customizable computing environment for running users&amp;#x0027; MPI applications in &amp;#x201C;hybrid&amp;#x201D; MPI mode-where the MPI on the host machine works in tandem with MPI inside the container-on NASA&amp;#x0027;s High-End Computing Capability (HECC) resources. Two types of real-world applications were tested: traditional High-Performance Computing (HPC) and Artificial Intelligence/Machine Learning (AI/ML). On the traditional HPC side, two JEDI containers built with Intel MPI for Earth science modeling were tested on both HECC in-house and HECC AWS Cloud CPU resources. On the AI/ML side, a NVIDIA TensorFlow container built with OpenMPI was tested with a Neural Collaborative Filtering recommender system and the ResNet-50 computer image system on the HECC in-house V100 GPUs. For each of these applications and resource environments, multiple hurdles were overcome after lengthy debugging efforts. Among them, the most significant ones were due to the conflicts between a host MPI and a container MPI and the complexity of the communication layers underneath. Although porting containers to run with a single node using just the container MPI is quite straightforward, our exercises demonstrate that running across multiple nodes in hybrid MPI mode requires knowledge of Singularity, MPI libraries, the operating system image, and the communication infrastructure such as the transport and network layers, which are traditionally handled by support staff of HPC centers and hardware or software vendors. In conclusion, porting and running Singularity containers on HECC resources or other data centers with similar environments is feasible but most users would need help to run them in hybrid MPI mode.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Chang and S. Heistand and R. Hood and H. Jin},
	booktitle = {2021 3rd International Workshop on Containers and New Orchestration Paradigms for Isolated Environments in HPC (CANOPIE-HPC)},
	date-added = {2022-10-09 18:16:36 -0700},
	date-modified = {2022-10-09 18:17:37 -0700},
	doi = {10.1109/CANOPIEHPC54579.2021.00007},
	keywords = {knowledge engineering;sockets;computational modeling;blades;operating systems;nasa;graphics processing units},
	month = {nov},
	pages = {17-28},
	publisher = {IEEE Computer Society},
	title = {Feasibility of Running Singularity Containers with Hybrid MPI on NASA High-End Computing Resources},
	url = {https://doi.ieeecomputersociety.org/10.1109/CANOPIEHPC54579.2021.00007},
	year = {2021},
	bdsk-url-1 = {https://doi.ieeecomputersociety.org/10.1109/CANOPIEHPC54579.2021.00007},
	bdsk-url-2 = {https://doi.org/10.1109/CANOPIEHPC54579.2021.00007}}

@misc{top500,
	author = {Hans Meuer and Erich Strohmaier and Jack Dongarra and Horst Simon},
	date-added = {2022-10-09 17:49:07 -0700},
	date-modified = {2022-10-09 17:49:07 -0700},
	howpublished = {Online. https://top500.org},
	title = {{Top500 Supercomputer Sites}},
	year = {2009},
	bdsk-url-1 = {http://www.top500.org}}

@inproceedings{gamblin+:sc15,
	acceptancerate = {22\%},
	address = {Austin, Texas},
	author = {Todd Gamblin and Matthew P. LeGendre and Michael R. Collette and Gregory L. Lee and Adam Moody and Bronis R. de Supinski and W. Scott Futral},
	booktitle = {Supercomputing 2015 (SC'15)},
	date-added = {2020-09-06 16:22:55 -0700},
	date-modified = {2021-10-06 16:15:22 -0700},
	month = {November 15-20},
	note = {LLNL-CONF-669890},
	title = {{The Spack Package Manager: Bringing order to HPC software chaos}},
	year = {2015},
	bdsk-url-1 = {http://tgamblin.github.io/pubs/spack-sc15.pdf}}

@inproceedings{hoste+:pyhpc12,
	author = {Hoste, Kenneth and Timmerman, Jens and Georges, Andy and De Weirdt, Stijn},
	booktitle = {High Performance Computing, Networking, Storage and Analysis, Proceedings},
	date-added = {2015-04-09 17:06:46 +0000},
	date-modified = {2015-07-31 11:38:51 +0000},
	keyword = {Python,automation,build procedure,installation,scientific software,compilation},
	language = {eng},
	location = {Salt Lake City, UT, USA},
	pages = {572--582},
	publisher = {IEEE},
	title = {{EasyBuild: Building Software with Ease}},
	url = {http://dx.doi.org/10.1109/SC.Companion.2012.81},
	year = {2012},
	bdsk-url-1 = {http://dx.doi.org/10.1109/SC.Companion.2012.81}}

@misc{jacamar-ci,
	author = {Paul Bryant},
	date-added = {2020-09-06 15:05:25 -0700},
	date-modified = {2022-10-09 17:57:31 -0700},
	howpublished = {Online: https://ecp-ci.gitlab.io/, https://gitlab.com/ecp-ci/jacamar-ci},
	title = {{Jacamar CI}},
	year = {2019-2022}}

@misc{NSF-XD,
	author = {{National Science Foundation}},
	howpublished = {https://www.nsf.gov/pubs/2008/nsf08571/nsf08571.htm},
	title = {{TeraGrid Phase III: eXtreme Digital Resources for Science and Engineering (XD)}},
	year = 2008}

@comment{BibDesk Static Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>group name</key>
		<string>software-integration</string>
		<key>keys</key>
		<string>gamblin+:sc15</string>
	</dict>
</array>
</plist>
}}
